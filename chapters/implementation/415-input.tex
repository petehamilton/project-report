\section{Input}

\subsection{Medium Selection}
\label{sec:medium-selection}

The first step in this project was to establish the best method for a student to interact with the application. As seen in \cref{sec:prior-research} a lot of different mediums for OMR have been tried over the years, however which one would be most suitable for a student to use wouldn't necessarily correlate with which was best for quality of scanning, speed of analysis etc.

\subsubsection{Flat Bed Scanner}

The most simple of all the input methods, this would involve a student writing on a sheet of manuscript paper, then using a flat bed scanner (the type commonly found as a standalone device and in multi-function printers) to input the sheet into the computer. This method allows for a scan with high and consistent light levels (minimising noise and light-dependent artefacts), minimal distortion due to paper curvature (as the scanner usually flattens the sheet) and a high resolution end image, typically 300-600dpi.

The application would process the sheet using traditional OMR techniques and then provide feedback to the student. This technique would therefore require ownership of both a scanner and a computer on which to install and run the application. Alternatively, the student could upload the scanned image to a web based service for analysis, removing the need to install software.

\subsubsection{Photograph/Camera}

\todo[inline,color=red]{Implementation - Photo/Camera}

Total free for all, colours and light levels vary, extraction from background, resolution variable, noise variable

Reference `Project 8 Final Report'

\subsubsection{Gestures}
\todo[inline,color=red]{Implementation - Gestures}

Use gestures combined with classification (see book and some papers).
Good for classifying individual entities but more like the old palm tablets, usually one note at a time

\subsubsection{Tablet Input}
\todo[inline,color=red]{Implementation - Tablet Input}

Finger or stylus work well tends to depend on the child, finger often felt more natural

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/photos/user-finger.jpg}
        \caption{Using a finger}
    \end{subfigure}
    \begin{subfigure}[b]{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{gfx/photos/user-stylus.jpg}
        \caption{Using a stylus}
    \end{subfigure}

    \caption{Examples of a student trying different tablet input methods}
\end{figure}

Chose this method in the end


\subsection{Capturing Strokes}
\label{sec:capturing-strokes}
To capture strokes drawn by the user, several listeners are attached to the HTML5 canvas element on which the manuscript is rendered. When the user presses their mouse down or initiates a touch event, a new array of line points is created and the initial point of interaction is stored in that array. From there, any mouse or touch movement whilst the touch or `mousedown' event is active triggers a new point recording, until the mouse button is released or the touch event ends. In this way, an array of points is built up which represent a drawn line.

Each time the line changes via the addition of new points, a redraw event is triggered which connects all the points together and renders the line onto the canvas.

In initial experiments with simple manuscript entities, this worked well, however as children began to experiment with more complex entities (requiring more lines and therefore more points) the length of time required to redraw the lines on the canvas became prohibitively long. Eventually a lag occurred between `pen' movement and lines being rendering on the canvas, the result of which was that as more lines were drawn, the time between new points being registered increased. This increase led to the drawing experience feeling (as one user described it) `really clunky' and long straight lines appearing instead of smooth curves (\cref{fig:drawing-lag-rough}).

\begin{figure}[hbt                                   ]
    \centering

    \begin{subfigure}[b]{.49\linewidth}
        \centering
      \includegraphics[width=\linewidth]{gfx/implementation/lag-stave.png}
      \caption{Drawing with refresh rendering - note that after drawing a high density region of many points, a spiral outward is very `angular' due to the additional time it takes to redraw so many points on every movement event }
      \label{fig:drawing-lag-rough}
    \end{subfigure}
    \begin{subfigure}[b]{.49\linewidth}
        \centering
      \includegraphics[width=\linewidth]{gfx/implementation/nolag-stave.png}
      \caption{Drawing with incremental rendering, not that after a similar high density region of points, the curve outwards still appears smooth}
      \label{fig:drawing-lag-smooth}
    \end{subfigure}

  \label{Drawing Lag}

\end{figure}

To counteract this, I modified the code such that it renders new line segments on the fly, rather than re-rendering all the lines every time a new point is added. Although this requires some more logic to render the line segment, but overall the experience turned out much smoother as the small increase in the length of the code path had much less impact than re-rendering the drawing every time (\cref{fig:drawing-lag-smooth}).

\section{Data Storage and Retrieval}

\subsection{Stave Drawing}
I store the data gathered in \cref{sec:capturing-strokes} in two ways.

Firstly, I store the serialized JSON of the stroke data in the database. This enables potential later on to experiment more with gesture based segmentation and recognition \todo[inline]{Reference Gesture based recognition}, showing a student corrections by transforming points in the strokes as opposed to transformations on the image. It also facilitates some great UI features like playing back the drawing for the tutor so they can see exactly how the student approached the problem and more ideas I express in \cref{sec:future-work}.

Secondly, I store the entire rendered canvas in the cloud using an Amazon S3 bucket, as a 'reference copy' in case I need to check the JSON against the original image at a later date or re-analyse images after updating my classifiers, features or analysis techniques.

\todo[inline]{Should I show my database schema here?}

\subsection{Components}

Once components have been extracted from the drawing using the techniques outlined in \cref{sec:identification}, I store both their features and \acrfull{RLE} representation of the whole component in the database for retrieval later during any further image processing.

Although 3000 components uses around 1GB in data when raw, using compression we are able to lower this considerably using run length encoding as shown in \ref{table:rle-improvement}.

\begin{table}[H]

    \begin{tabularx}{\textwidth}{ X X X X }
    \toprule
    Metric                  & Without RLE   & With RLE   & Improvement \\
    \midrule
    Storage Required        & 759 MB        & 22 MB      & 91.7\%      \\
    Total Retrieval Time    & 1639 ms       & 42 ms      & 97.4\% \\
    \bottomrule
    \end{tabularx}

    \caption{To improve the speed of my application, I utilised \acrfull{RLE} (covered in \cref{sec:tb-rle}) to improve storage and retrieval times during feature extraction and classification by up to 97\% (\cref{table:rle-improvement}).}
    \label{table:rle-improvement}
\end{table}

\subsection{Database Schema}

\subsection{Entity Relationship Diagram}





