\section{Classification}

Statistic (parametric and non-parametric) vs Syntactic

\subsection{Normalized Cross Correlation}

\subsection{Neural Network}
Possibly drop this, I didn't use it and it rapidly decreased in effectiveness

\subsection{Nearest Neighbour}

The \acrfull{KNN} algorithm is an example of instance based learning and theoretically doesn't require supervision or training.

KNN classification works by extracting a set of features for each sample and using this `feature vector' to represent each sample in a multidimensional vector space.

Once this space has been generated, new samples may be classified by generating their feature vector and calculating the $k$ closest or `most similar' samples in the model by way of a distance calculation. By considering the $k$ closest samples, a consensus can be reached and the new sample is classified according to the label with the greatest majority in the $k$ nearest neighbours as seen in \ref{fig:knn-example}.

 \begin{figure}[h!]
   \centering
%   \includegraphics[draft, width=\linewidth]{gfx/background-omr/knn-example.png}
    \missingfigure{Add a KNN Example Image}
   \caption{KNN example using musical symbols}
   \label{fig:knn-example}
 \end{figure}

The two most critical aspects of the KNN classifier are the distance method used and the number of neighbours considered. In order to maximise the accuracy of the classifier I perform several experiments to this effect in section \ref{sec:implementation-classification}

\subsection{Modified Nearest Neighbour}

To save space, improve accuracy etc, note the issues I had with storage space, speed of classifier generation.

Just do wilson and wilson iterative
